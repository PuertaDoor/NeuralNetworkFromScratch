{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9ed66a-801c-4b57-b275-9687646b9b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc294a-f1a6-42f1-b5e3-e3f59c6dc4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(object):\n",
    "    def forward(self, y, yhat):\n",
    "        pass\n",
    "\n",
    "    def backward(self, y, yhat):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f119726b-6d55-47f9-b640-39326c19a3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSELoss(Loss):\n",
    "    def forward(self, y, yhat):\n",
    "        return np.linalg.norm(y - yhat, axis=1)**2 # axis=1 to have batch vectors\n",
    "    \n",
    "    def backward(self, y, yhat):\n",
    "        return -2 * (y - yhat) # taille batch*d, gradient de la MSE en fonction des yhat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5f6b96-6a6e-4369-bfb9-40a0e14093b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CE(Loss):\n",
    "    def forward(self, y, yhat): # y and yhat are matrix\n",
    "        onehot = np.zeros((y.shape[1],10))\n",
    "        onehot[np.arange(y.size),y]=1\n",
    "        \n",
    "        return np.array([-yhat[i][onehot[i]==1] for i in range(y.shape[0])])\n",
    "    \n",
    "    def backward(self, y, yhat):\n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193e6701-f57b-4874-befe-c2054a293378",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def __init__(self):\n",
    "        self._parameters = None\n",
    "        self._gradient = None\n",
    "\n",
    "    def zero_grad(self):\n",
    "        ## Annule gradient\n",
    "        pass\n",
    "\n",
    "    def forward(self, X):\n",
    "        ## Calcule la passe forward\n",
    "        pass\n",
    "\n",
    "    def update_parameters(self, gradient_step=1e-3):\n",
    "        ## Calcule la mise a jour des parametres selon le gradient calcule et le pas de gradient_step\n",
    "        self._parameters -= gradient_step*self._gradient\n",
    "\n",
    "    def backward_update_gradient(self, input, delta):\n",
    "        ## Met a jour la valeur du gradient\n",
    "        pass\n",
    "\n",
    "    def backward_delta(self, input, delta):\n",
    "        ## Calcul la derivee de l'erreur\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e911ef8f-3311-486f-b0cb-ac0e3125a7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):\n",
    "    def __init__(self, input, output):\n",
    "        super().__init__(self)\n",
    "        self.input = input\n",
    "        self.output = output\n",
    "        self._parameters = np.zeros((input, output)) # (batch*input @ input*output) -> (batch*output)\n",
    "        self._gradient = np.zeros(self._parameters.shape)\n",
    "    \n",
    "    def zero_grad(self): # reinitialise à 0 le gradient\n",
    "        return np.zeros(self._parameters.shape)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        return X @ self._parameters # sans biais\n",
    "        \n",
    "    def backward_update_gradient(self, X, delta):\n",
    "        self._gradient += X.T @ delta\n",
    "        \n",
    "    def backward_delta(self, X, delta):\n",
    "        return delta @ self._parameters.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8561a708-2ae5-449d-89e8-4369fb17182b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TanH(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(self)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return np.tanh(X) # transformation\n",
    "    \n",
    "    def backward_update_gradient(self, X, delta):\n",
    "        return 1 - self.forward(X)**2 * delta # d'après la dérivée en fonction de X de tanh(X), delta joue le rôle de constante.\n",
    "    \n",
    "    def update_parameters(self, gradient_step=1e-3):\n",
    "        pass # nous n'avons pas de paramètre dans ce modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78351d68-59d4-40df-9535-c8f4c9e49da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoide(X):\n",
    "    return 1/(1+np.exp(-X)) # sigmoide n'existe pas dans NumPy, nous l'implémentons ici.\n",
    "\n",
    "class Sigmoide(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(self)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        return sigmoide(X)\n",
    "    \n",
    "    def backward_update_gradient(self, X, delta):\n",
    "        return (np.exp(-X) * sigmoide(X) / (1+np.exp(-X))) * delta # d'après la dérivée en fonction de X de sigmoide(X) (avec une constante lambda de 1), delta joue le rôle de constante.\n",
    "    \n",
    "    def update_parameters(self, gradient_step=1e-3):\n",
    "        pass # nous n'avons pas de paramètre dans ce modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d918b49c-2041-41d2-9709-22e7247de1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__(self)\n",
    "        \n",
    "    def forward(self, Z): # Z is a vector\n",
    "        e = np.exp(Z)\n",
    "        return e / np.sum(e)\n",
    "    \n",
    "    def backward_update_gradient(self, Z, delta): # Jacobian of Softmax function\n",
    "        K = Z.size\n",
    "        jcb_softmax = np.zeros((K,K))\n",
    "        softmax = self.forward(Z)\n",
    "        for i in range(K):\n",
    "            for j in range(K):\n",
    "                if i==j:\n",
    "                    jcb_softmax[i][j] = softmax[i] * (1 - softmax[i])\n",
    "                else:\n",
    "                    jcb_softmax[i][j] = - softmax[i] * softmax[j]\n",
    "                    \n",
    "        return jcb_softmax\n",
    "    \n",
    "    def update_parameters(self, gradient_step=1e-3):\n",
    "        pass # nous n'avons pas de paramètre dans ce modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5c1e35-22d8-4819-9019-08e684766c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentiel(object):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314db4a2-0325-404e-9494-1e0e75d0a77c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
